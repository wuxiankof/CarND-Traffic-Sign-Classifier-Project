{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Load The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import utilities_tf1 as ut\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import time\n",
    "%config InlineBackend.figure_format = 'svg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Load pickled data\n",
    "\n",
    "# TODO: Fill this in based on where you saved the training and testing data\n",
    "\n",
    "training_file = '../data/train.p'\n",
    "validation_file= '../data/valid.p'\n",
    "testing_file = '../data/test.p'\n",
    "\n",
    "with open(training_file, mode='rb') as f:\n",
    "    train = pickle.load(f)\n",
    "with open(validation_file, mode='rb') as f:\n",
    "    valid = pickle.load(f)\n",
    "with open(testing_file, mode='rb') as f:\n",
    "    test = pickle.load(f)\n",
    "    \n",
    "X_train, y_train = train['features'], train['labels']\n",
    "X_valid, y_valid = valid['features'], valid['labels']\n",
    "X_test, y_test = test['features'], test['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (34799, 32, 32, 3)\n",
      "X_valid shape: (4410, 32, 32, 3)\n",
      "X_test shape: (12630, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "print ('X_train shape:', X_train.shape)\n",
    "print ('X_valid shape:', X_valid.shape)\n",
    "print ('X_test shape:', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = ut.pre_process(X_train, channels=1)\n",
    "X_valid = ut.pre_process(X_valid, channels=1)\n",
    "X_test = ut.pre_process(X_test, channels=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Pre-processing:\n",
      "X_train shape: (34799, 32, 32, 1)\n",
      "X_valid shape: (4410, 32, 32, 1)\n",
      "X_test shape: (12630, 32, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "print ('After Pre-processing:')\n",
    "print ('X_train shape:', X_train.shape)\n",
    "print ('X_valid shape:', X_valid.shape)\n",
    "print ('X_test shape:', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Model (1): with original training/validation/test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'Input_channels': 1, \n",
    "    'n_classes': len(np.unique(y_train)), \n",
    "    'EPOCHS': 20, \n",
    "    'BATCH_SIZE': 128, \n",
    "    'rate': 0.001, \n",
    "    'training_operation': None,\n",
    "    'accuracy_operation': None, \n",
    "    'x': None, \n",
    "    'y': None\n",
    "}\n",
    "\n",
    "Data = {\n",
    "    'X_train': X_train,\n",
    "    'y_train': y_train,\n",
    "    'X_valid': X_valid,\n",
    "    'y_valid': y_valid,\n",
    "    'X_test': X_test,\n",
    "    'y_test': y_test\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ut.Compile_Model(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "\n",
      "EPOCH 5 ...\n",
      "Training Accuracy = 0.881\n",
      "Validation Accuracy = 0.702\n",
      "Testing Accuracy = 0.705\n",
      "\n",
      "EPOCH 10 ...\n",
      "Training Accuracy = 0.951\n",
      "Validation Accuracy = 0.748\n",
      "Testing Accuracy = 0.752\n",
      "\n",
      "EPOCH 15 ...\n",
      "Training Accuracy = 0.964\n",
      "Validation Accuracy = 0.754\n",
      "Testing Accuracy = 0.746\n",
      "\n",
      "EPOCH 20 ...\n",
      "Training Accuracy = 0.972\n",
      "Validation Accuracy = 0.758\n",
      "Testing Accuracy = 0.762\n",
      "\n",
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "ut.Train_and_Test_Model(params, Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion**\n",
    "\n",
    "the Training/evaluation results can be summaried in the table below: \n",
    "\n",
    "|Dataset|Accuracy (%)|\n",
    "|-|-|\n",
    "|Training set|97.2%|\n",
    "|Validation set|75.8%|\n",
    "|Test set|76.2%|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion: \n",
    "\n",
    "- If the validation/test data comes from the same distribution as the training set, then there is a large variance problem and the algorithm is not generalizing well from the training set.\n",
    "- However, if training data and the validation/test data come from  different distributions, there isn't necessarily a variance/overfitting problem. The problem might be that the validation/test set contains images that are more difficult to classify accurately.\n",
    "- It's difficult to know whethere it is the abovementioned issue produces this approx 22% difference in accuracy between the training set and the validation/test set. \n",
    "- To resolve this issue, I define a new subset called `training-validation set`. This new subset has the same distribution as the training set, but it is not used for training the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Model (2): with training-validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (27839, 32, 32, 1)\n",
      "X_valid shape: (6960, 32, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "# training-validation data set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, \n",
    "                                                      test_size=0.2, random_state=0)\n",
    "\n",
    "print ('X_train shape:', X_train.shape)\n",
    "print ('X_valid shape:', X_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'Input_channels': 1, \n",
    "    'n_classes': len(np.unique(y_train)), \n",
    "    'EPOCHS': 20, \n",
    "    'BATCH_SIZE': 128, \n",
    "    'rate': 0.001, \n",
    "    'training_operation': None,\n",
    "    'accuracy_operation': None, \n",
    "    'x': None, \n",
    "    'y': None\n",
    "}\n",
    "\n",
    "Data = {\n",
    "    'X_train': X_train,\n",
    "    'y_train': y_train,\n",
    "    'X_valid': X_valid,\n",
    "    'y_valid': y_valid,\n",
    "    'X_test': X_test,\n",
    "    'y_test': y_test\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ut.Compile_Model(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "\n",
      "EPOCH 5 ...\n",
      "Training Accuracy = 0.837\n",
      "Training-validation Accuracy = 0.798\n",
      "Testing Accuracy = 0.650\n",
      "\n",
      "EPOCH 10 ...\n",
      "Training Accuracy = 0.925\n",
      "Training-validation Accuracy = 0.874\n",
      "Testing Accuracy = 0.701\n",
      "\n",
      "EPOCH 15 ...\n",
      "Training Accuracy = 0.966\n",
      "Training-validation Accuracy = 0.910\n",
      "Testing Accuracy = 0.726\n",
      "\n",
      "EPOCH 20 ...\n",
      "Training Accuracy = 0.977\n",
      "Training-validation Accuracy = 0.920\n",
      "Testing Accuracy = 0.734\n",
      "\n",
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "ut.Train_and_Test_Model(params, Data, train_validation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion**\n",
    "\n",
    "the Training/evaluation results can be summaried in the table below: \n",
    "\n",
    "|Dataset|Accuracy (%)|\n",
    "|-|-|\n",
    "|Training set|97.7%|\n",
    "|Training-Validation set|92.0%|\n",
    "|Validation set (from previous run)|75.8%|\n",
    "|Test set|73.4%|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "- From the analysis above, it can be concluded that the training set and validation/test set are very likely from diffierent distributions, which indicates that there might be a data dismaching issue. \n",
    "- To solve this problem, I have conducted a trail to add a portion of the test set data to the training set which is to enable the model to learn features of the test set. this has been illustrated in the main jupyter notebook \"Traffic_Sign_Classifier_WX.ipynb\" in the same folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
